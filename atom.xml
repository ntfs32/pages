<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://gois.top</id>
    <title>Shaddock</title>
    <updated>2019-08-31T03:19:45.599Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://gois.top"/>
    <link rel="self" href="https://gois.top/atom.xml"/>
    <subtitle>æ¸©æ•…è€ŒçŸ¥æ–°</subtitle>
    <logo>https://gois.top/images/avatar.png</logo>
    <icon>https://gois.top/favicon.ico</icon>
    <rights>All rights reserved 2019, Shaddock</rights>
    <entry>
        <title type="html"><![CDATA[CentOS 7.6å®‰è£…rancher V2.2.8 é«˜å¯ç”¨é›†ç¾¤]]></title>
        <id>https://gois.top/post/centos-76-an-zhuang-rancher-v228-gao-ke-yong-ji-qun</id>
        <link href="https://gois.top/post/centos-76-an-zhuang-rancher-v228-gao-ke-yong-ji-qun">
        </link>
        <updated>2019-08-31T03:18:37.000Z</updated>
        <content type="html"><![CDATA[<h1 id="centos-76å®‰è£…rancher-v228-é«˜å¯ç”¨é›†ç¾¤">CentOS 7.6å®‰è£…rancher V2.2.8 é«˜å¯ç”¨é›†ç¾¤</h1>
<ul>
<li>ä½¿ç”¨GCPä¸»æœºä½œä¸ºå®‰è£…ç¤ºä¾‹</li>
</ul>
<pre><code class="language-bash">[rke@rancher-1 ~]$ cat /etc/redhat-release
CentOS Linux release 7.6.1810 (Core)
</code></pre>
<table>
<thead>
<tr>
<th>ç”¨æˆ·å</th>
<th>ä¸»æœºå</th>
<th>å†…ç½‘IP</th>
<th>å…¬ç½‘IP</th>
<th>SSHç«¯å£</th>
</tr>
</thead>
<tbody>
<tr>
<td>rke</td>
<td>rancher-1</td>
<td>10.128.0.2</td>
<td>xxx</td>
<td>22</td>
</tr>
<tr>
<td>rke</td>
<td>rancher-2</td>
<td>10.128.0.3</td>
<td>xxx</td>
<td>22</td>
</tr>
<tr>
<td>rke</td>
<td>rancher-3</td>
<td>10.128.0.4</td>
<td>xxx</td>
<td>22</td>
</tr>
</tbody>
</table>
<ol>
<li>ä¸´æ—¶å…³é—­selinux</li>
</ol>
<pre><code class="language-bash">sudo setenforce 0
</code></pre>
<ol start="2">
<li>æ°¸ä¹…å…³é—­selinux</li>
</ol>
<pre><code class="language-bash">sudo  sed -i 's/SELINUX=enforcing/SELINUX=disabled/'  /etc/sysconfig/selinux
</code></pre>
<ol start="3">
<li>å®‰è£…docker</li>
</ol>
<pre><code class="language-bash">sudo yum install -y yum-utils wget device-mapper-persistent-data lvm2

sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

sudo yum install -y docker-ce-18.09.8 docker-ce-18.09.8 containerd.io

sudo usermod -aG docker ${USER}

sudo systemctl start docker

sudo systemctl enable docker

</code></pre>
<ol start="5">
<li>ä¸‹è½½é•œåƒ</li>
</ol>
<pre><code class="language-bash">wget https://github.com/rancher/rke/releases/download/v0.2.8/rke_linux-amd64
sudo chmod 777 rke_linux-amd64 &amp;&amp; sudo cp rke_linux-amd64 /usr/local/bin/rke
wget https://github.com/rancher/rancher/releases/download/v2.2.8/rancher-images.txt
wget https://github.com/rancher/rancher/releases/download/v2.2.8/rancher-save-images.sh
chmod 755 rancher-save-images.sh
#ä¸è¿è¡Œdocker save
sed -i 's/docker save/#docker save/g' rancher-save-images.sh
./rancher-save-images.sh
</code></pre>
<ol start="6">
<li>é…ç½®hosts,å¹¶æµ‹è¯•sshå…å¯†è®¿é—®å„ä¸ªä¸»æœº(rancher-1ä¸Šå®‰è£…çš„è¯ï¼ŒåŒ…æ‹¬å…å¯†è®¿é—®è‡ªå·±)</li>
</ol>
<pre><code class="language-bash">10.128.0.2  rancher-1
10.128.0.3  rancher-2
10.128.0.4  rancher-3
</code></pre>
<pre><code class="language-bash">ssh rancher-1
</code></pre>
<ol start="7">
<li>
<p>ä¸‹è½½helm</p>
<pre><code class="language-bash">wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz
tar zxf helm-v2.14.3-linux-amd64.tar.gz
chmod +x ./linux-amd64/helm &amp;&amp; sudo cp ./linux-amd64/helm /usr/local/bin/
</code></pre>
</li>
<li>
<p>ä¸‹è½½kubectl</p>
<pre><code class="language-bash"># æœ‰å¢™å“¦
# https://storage.googleapis.com/kubernetes-release/release/v1.14.5/bin/linux/amd64/kubectl
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
chmod +x ./kubectl &amp;&amp; sudo cp ./kubectl /usr/local/bin/
</code></pre>
</li>
<li>
<p>åˆå§‹åŒ–rkeé…ç½®æ–‡ä»¶</p>
</li>
</ol>
<pre><code class="language-yaml">nodes:
- address: 10.128.0.2
  #internal_address: 104.198.227.73
  user: rke
  role: [controlplane, etcd, worker]
  ssh_key_path: /home/rke/.ssh/id_rsa
- address: 10.128.0.3
  #internal_address: 35.232.38.212
  user: rke
  role: [controlplane, etcd, worker]
  ssh_key_path: /home/rke/.ssh/id_rsa
- address: 10.128.0.4
  #internal_address: 35.232.158.150
  user: rke
  role: [controlplane, etcd, worker]
  ssh_key_path: /home/rke/.ssh/id_rsa

services:
  etcd:
    snapshot: true
    creation: 6h
    retention: 24h
</code></pre>
<ol start="10">
<li>é…ç½®è§£æåˆ°å„ä¸ªèŠ‚ç‚¹, (å¦‚æœèŠ‚ç‚¹æœ‰å…¬ç½‘IPæˆ–è€…åªæ‰“ç®—å†…ç½‘è®¿é—®ï¼Œè¿™æ ·å°±å¯ä»¥äº†ï¼Œå¦åˆ™å°±åƒå®˜æ–¹æ–‡æ¡£é‚£æ ·ï¼Œé…ç½®nginxä»£ç†å§), ä¾‹å¦‚ï¼š</li>
</ol>
<pre><code class="language-bash">rancher.xxx.top A 104.177.227.73
rancher.xxx.top A 35.111.38.212
rancher.xxx.top A 35.111.158.150
</code></pre>
<ol start="11">
<li>rkeå®‰è£…é›†ç¾¤</li>
</ol>
<pre><code class="language-bash">rke up --config ./rancher-cluster.yml
</code></pre>
<pre><code class="language-bash">[rke@rancher-1 ~]$ rke up --config ./rancher-cluster.yml
INFO[0000] Initiating Kubernetes cluster
INFO[0000] [certificates] Generating Kubernetes API server certificates
INFO[0000] [certificates] Generating admin certificates and kubeconfig
INFO[0000] [certificates] Generating etcd-10.128.0.2 certificate and key
INFO[0000] [certificates] Generating etcd-10.128.0.3 certificate and key
INFO[0000] [certificates] Generating etcd-10.128.0.4 certificate and key
INFO[0000] Successfully Deployed state file at [./rancher-cluster.rkestate]
INFO[0000] Building Kubernetes cluster
INFO[0000] [dialer] Setup tunnel for host [10.128.0.2]
INFO[0000] [dialer] Setup tunnel for host [10.128.0.4]
INFO[0000] [dialer] Setup tunnel for host [10.128.0.3]
INFO[0000] [network] Deploying port listener containers
INFO[0001] [network] Successfully started [rke-etcd-port-listener] container on host [10.128.0.4]
INFO[0001] [network] Successfully started [rke-etcd-port-listener] container on host [10.128.0.3]
INFO[0001] [network] Successfully started [rke-etcd-port-listener] container on host [10.128.0.2]
INFO[0001] [network] Successfully started [rke-cp-port-listener] container on host [10.128.0.2]
INFO[0001] [network] Successfully started [rke-cp-port-listener] container on host [10.128.0.3]
INFO[0001] [network] Successfully started [rke-cp-port-listener] container on host [10.128.0.4]
INFO[0002] [network] Successfully started [rke-worker-port-listener] container on host [10.128.0.4]
INFO[0002] [network] Successfully started [rke-worker-port-listener] container on host [10.128.0.2]
INFO[0002] [network] Successfully started [rke-worker-port-listener] container on host [10.128.0.3]
INFO[0002] [network] Port listener containers deployed successfully
INFO[0002] [network] Running etcd &lt;-&gt; etcd port checks
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.4]
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.3]
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.2]
INFO[0002] [network] Running control plane -&gt; etcd port checks
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.2]
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.3]
INFO[0002] [network] Successfully started [rke-port-checker] container on host [10.128.0.4]
INFO[0002] [network] Running control plane -&gt; worker port checks
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.4]
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.3]
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.2]
INFO[0003] [network] Running workers -&gt; control plane port checks
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.2]
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.4]
INFO[0003] [network] Successfully started [rke-port-checker] container on host [10.128.0.3]
INFO[0003] [network] Checking KubeAPI port Control Plane hosts
INFO[0003] [network] Removing port listener containers
INFO[0003] [remove/rke-etcd-port-listener] Successfully removed container on host [10.128.0.4]
INFO[0003] [remove/rke-etcd-port-listener] Successfully removed container on host [10.128.0.2]
INFO[0003] [remove/rke-etcd-port-listener] Successfully removed container on host [10.128.0.3]
INFO[0004] [remove/rke-cp-port-listener] Successfully removed container on host [10.128.0.4]
INFO[0004] [remove/rke-cp-port-listener] Successfully removed container on host [10.128.0.2]
INFO[0004] [remove/rke-cp-port-listener] Successfully removed container on host [10.128.0.3]
INFO[0004] [remove/rke-worker-port-listener] Successfully removed container on host [10.128.0.4]
INFO[0004] [remove/rke-worker-port-listener] Successfully removed container on host [10.128.0.2]
INFO[0004] [remove/rke-worker-port-listener] Successfully removed container on host [10.128.0.3]
INFO[0004] [network] Port listener containers removed successfully
INFO[0004] [certificates] Deploying kubernetes certificates to Cluster nodes
INFO[0009] [reconcile] Rebuilding and updating local kube config
INFO[0009] Successfully Deployed local admin kubeconfig at [./kube_config_rancher-cluster.yml]
INFO[0009] Successfully Deployed local admin kubeconfig at [./kube_config_rancher-cluster.yml]
INFO[0009] Successfully Deployed local admin kubeconfig at [./kube_config_rancher-cluster.yml]
INFO[0009] [certificates] Successfully deployed kubernetes certificates to Cluster nodes
INFO[0009] [reconcile] Reconciling cluster state
INFO[0009] [reconcile] This is newly generated cluster
INFO[0009] Pre-pulling kubernetes images
INFO[0009] Kubernetes images pulled successfully
INFO[0009] [etcd] Building up etcd plane..
INFO[0010] Waiting for [etcd] container to exit on host [10.128.0.2]
INFO[0010] [etcd] Successfully updated [etcd] container on host [10.128.0.2]
INFO[0010] [etcd] Saving snapshot [etcd-rolling-snapshots] on host [10.128.0.2]
INFO[0010] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.128.0.2]
INFO[0010] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.128.0.2]
INFO[0016] [certificates] Successfully started [rke-bundle-cert] container on host [10.128.0.2]
INFO[0016] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.2]
INFO[0016] Container [rke-bundle-cert] is still running on host [10.128.0.2]
INFO[0017] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.2]
INFO[0017] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.128.0.2]
INFO[0017] [etcd] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0017] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0018] Waiting for [etcd] container to exit on host [10.128.0.3]
INFO[0018] [etcd] Successfully updated [etcd] container on host [10.128.0.3]
INFO[0018] [etcd] Saving snapshot [etcd-rolling-snapshots] on host [10.128.0.3]
INFO[0018] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.128.0.3]
INFO[0018] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.128.0.3]
INFO[0024] [certificates] Successfully started [rke-bundle-cert] container on host [10.128.0.3]
INFO[0024] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.3]
INFO[0024] Container [rke-bundle-cert] is still running on host [10.128.0.3]
INFO[0025] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.3]
INFO[0025] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.128.0.3]
INFO[0025] [etcd] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0025] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0026] Waiting for [etcd] container to exit on host [10.128.0.4]
INFO[0026] [etcd] Successfully updated [etcd] container on host [10.128.0.4]
INFO[0026] [etcd] Saving snapshot [etcd-rolling-snapshots] on host [10.128.0.4]
INFO[0026] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.128.0.4]
INFO[0026] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.128.0.4]
INFO[0032] [certificates] Successfully started [rke-bundle-cert] container on host [10.128.0.4]
INFO[0032] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.4]
INFO[0032] Container [rke-bundle-cert] is still running on host [10.128.0.4]
INFO[0033] Waiting for [rke-bundle-cert] container to exit on host [10.128.0.4]
INFO[0033] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.128.0.4]
INFO[0033] [etcd] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0033] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0033] [etcd] Successfully started etcd plane.. Checking etcd cluster health
INFO[0034] [controlplane] Building up Controller Plane..
INFO[0034] [controlplane] Successfully started [kube-apiserver] container on host [10.128.0.3]
INFO[0034] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.128.0.3]
INFO[0034] [controlplane] Successfully started [kube-apiserver] container on host [10.128.0.4]
INFO[0034] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.128.0.4]
INFO[0034] [controlplane] Successfully started [kube-apiserver] container on host [10.128.0.2]
INFO[0034] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.128.0.2]
INFO[0044] [healthcheck] service [kube-apiserver] on host [10.128.0.3] is healthy
INFO[0044] [healthcheck] service [kube-apiserver] on host [10.128.0.4] is healthy
INFO[0044] [healthcheck] service [kube-apiserver] on host [10.128.0.2] is healthy
INFO[0045] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0045] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0045] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0045] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0045] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0045] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0045] [controlplane] Successfully started [kube-controller-manager] container on host [10.128.0.2]
INFO[0045] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.128.0.2]
INFO[0045] [controlplane] Successfully started [kube-controller-manager] container on host [10.128.0.4]
INFO[0045] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.128.0.4]
INFO[0045] [controlplane] Successfully started [kube-controller-manager] container on host [10.128.0.3]
INFO[0045] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.128.0.3]
INFO[0051] [healthcheck] service [kube-controller-manager] on host [10.128.0.2] is healthy
INFO[0051] [healthcheck] service [kube-controller-manager] on host [10.128.0.4] is healthy
INFO[0051] [healthcheck] service [kube-controller-manager] on host [10.128.0.3] is healthy
INFO[0051] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0051] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0051] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0051] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0051] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0052] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0052] [controlplane] Successfully started [kube-scheduler] container on host [10.128.0.2]
INFO[0052] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.128.0.2]
INFO[0052] [controlplane] Successfully started [kube-scheduler] container on host [10.128.0.4]
INFO[0052] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.128.0.4]
INFO[0052] [controlplane] Successfully started [kube-scheduler] container on host [10.128.0.3]
INFO[0052] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.128.0.3]
INFO[0057] [healthcheck] service [kube-scheduler] on host [10.128.0.2] is healthy
INFO[0057] [healthcheck] service [kube-scheduler] on host [10.128.0.4] is healthy
INFO[0057] [healthcheck] service [kube-scheduler] on host [10.128.0.3] is healthy
INFO[0057] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0058] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0058] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0058] [controlplane] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0058] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0058] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0058] [controlplane] Successfully started Controller Plane..
INFO[0058] [authz] Creating rke-job-deployer ServiceAccount
INFO[0058] [authz] rke-job-deployer ServiceAccount created successfully
INFO[0058] [authz] Creating system:node ClusterRoleBinding
INFO[0058] [authz] system:node ClusterRoleBinding created successfully
INFO[0058] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding
INFO[0058] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully
INFO[0058] Successfully Deployed state file at [./rancher-cluster.rkestate]
INFO[0058] [state] Saving full cluster state to Kubernetes
INFO[0058] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: cluster-state
INFO[0058] [worker] Building up Worker Plane..
INFO[0058] [sidekick] Sidekick container already created on host [10.128.0.3]
INFO[0058] [sidekick] Sidekick container already created on host [10.128.0.2]
INFO[0058] [sidekick] Sidekick container already created on host [10.128.0.4]
INFO[0058] [worker] Successfully started [kubelet] container on host [10.128.0.2]
INFO[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.128.0.2]
INFO[0058] [worker] Successfully started [kubelet] container on host [10.128.0.4]
INFO[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.128.0.4]
INFO[0058] [worker] Successfully started [kubelet] container on host [10.128.0.3]
INFO[0058] [healthcheck] Start Healthcheck on service [kubelet] on host [10.128.0.3]
INFO[0064] [healthcheck] service [kubelet] on host [10.128.0.2] is healthy
INFO[0064] [healthcheck] service [kubelet] on host [10.128.0.4] is healthy
INFO[0064] [healthcheck] service [kubelet] on host [10.128.0.3] is healthy
INFO[0064] [worker] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0064] [worker] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0064] [worker] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0064] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0064] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0064] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0064] [worker] Successfully started [kube-proxy] container on host [10.128.0.2]
INFO[0064] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.128.0.2]
INFO[0065] [worker] Successfully started [kube-proxy] container on host [10.128.0.3]
INFO[0065] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.128.0.3]
INFO[0065] [worker] Successfully started [kube-proxy] container on host [10.128.0.4]
INFO[0065] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.128.0.4]
INFO[0065] [healthcheck] service [kube-proxy] on host [10.128.0.3] is healthy
INFO[0065] [healthcheck] service [kube-proxy] on host [10.128.0.4] is healthy
INFO[0065] [worker] Successfully started [rke-log-linker] container on host [10.128.0.4]
INFO[0065] [worker] Successfully started [rke-log-linker] container on host [10.128.0.3]
INFO[0065] [remove/rke-log-linker] Successfully removed container on host [10.128.0.4]
INFO[0065] [remove/rke-log-linker] Successfully removed container on host [10.128.0.3]
INFO[0070] [healthcheck] service [kube-proxy] on host [10.128.0.2] is healthy
INFO[0070] [worker] Successfully started [rke-log-linker] container on host [10.128.0.2]
INFO[0071] [remove/rke-log-linker] Successfully removed container on host [10.128.0.2]
INFO[0071] [worker] Successfully started Worker Plane..
INFO[0071] [cleanup] Successfully started [rke-log-cleaner] container on host [10.128.0.2]
INFO[0071] [cleanup] Successfully started [rke-log-cleaner] container on host [10.128.0.4]
INFO[0071] [cleanup] Successfully started [rke-log-cleaner] container on host [10.128.0.3]
INFO[0071] [remove/rke-log-cleaner] Successfully removed container on host [10.128.0.2]
INFO[0071] [remove/rke-log-cleaner] Successfully removed container on host [10.128.0.4]
INFO[0071] [remove/rke-log-cleaner] Successfully removed container on host [10.128.0.3]
INFO[0071] [sync] Syncing nodes Labels and Taints
INFO[0071] [sync] Successfully synced nodes Labels and Taints
INFO[0071] [network] Setting up network plugin: canal
INFO[0071] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes
INFO[0071] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes
INFO[0071] [addons] Executing deploy job rke-network-plugin
INFO[0076] [addons] Setting up coredns
INFO[0076] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes
INFO[0076] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes
INFO[0076] [addons] Executing deploy job rke-coredns-addon
INFO[0081] [addons] CoreDNS deployed successfully..
INFO[0081] [dns] DNS provider coredns deployed successfully
INFO[0081] [addons] Setting up Metrics Server
INFO[0081] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes
INFO[0082] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes
INFO[0082] [addons] Executing deploy job rke-metrics-addon
INFO[0087] [addons] Metrics Server deployed successfully
INFO[0087] [ingress] Setting up nginx ingress controller
INFO[0087] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes
INFO[0087] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes
INFO[0087] [addons] Executing deploy job rke-ingress-controller
INFO[0092] [ingress] ingress controller nginx deployed successfully
INFO[0092] [addons] Setting up user addons
INFO[0092] [addons] no user addons defined
INFO[0092] Finished building Kubernetes cluster successfully
</code></pre>
<ol start="12">
<li>æµ‹è¯•å®‰è£…å¥½çš„é›†ç¾¤</li>
</ol>
<pre><code class="language-bash">    export KUBECONFIG=$(pwd)/kube_config_rancher-cluster.yml
    kubectl get nodes
</code></pre>
<pre><code class="language-bash">NAME         STATUS   ROLES                      AGE   VERSION
10.128.0.2   Ready    controlplane,etcd,worker   64s   v1.14.6
10.128.0.3   Ready    controlplane,etcd,worker   64s   v1.14.6
10.128.0.4   Ready    controlplane,etcd,worker   64s   v1.14.6
</code></pre>
<ol start="13">
<li>å®‰è£…helm tiller</li>
</ol>
<pre><code class="language-bash">
kubectl -n kube-system create serviceaccount tiller

kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

helm init --service-account tiller

#Users in China: You will need to specify a specific tiller-image in order to initialize tiller. 

#The list of tiller image tags are available here: https://dev.aliyun.com/detail.html?spm=5176.1972343.2.18.ErFNgC&amp;repoId=62085. 

#When initializing tiller, you'll need to pass in --tiller-image

#tag v2.14.3 20190824
helm init --service-account tiller \
--tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3

</code></pre>
<pre><code class="language-bash">[rke@rancher-1 ~]$ kubectl -n kube-system get pod
NAME                                      READY   STATUS      RESTARTS   AGE
canal-bp9hw                               2/2     Running     0          4m22s
canal-d84cx                               2/2     Running     0          4m22s
canal-kftk2                               2/2     Running     0          4m22s
coredns-autoscaler-5d5d49b8ff-4h8f5       1/1     Running     0          4m17s
coredns-bdffbc666-mlc44                   1/1     Running     0          4m18s
metrics-server-7f6bd4c888-cqw2k           1/1     Running     0          4m13s
rke-coredns-addon-deploy-job-s78h6        0/1     Completed   0          4m19s
rke-ingress-controller-deploy-job-f69k8   0/1     Completed   0          4m9s
rke-metrics-addon-deploy-job-8ncvb        0/1     Completed   0          4m14s
rke-network-plugin-deploy-job-sntgn       0/1     Completed   0          4m25s
tiller-deploy-7f4d76c4b6-qdckk            1/1     Running     0          79s
[rke@rancher-1 ~]$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.14.3&quot;, GitCommit:&quot;0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.14.3&quot;, GitCommit:&quot;0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<ol start="14">
<li>å®‰è£…rancher chart</li>
</ol>
<pre><code class="language-bash">helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
</code></pre>
<ol start="15">
<li>å®‰è£… cert manager</li>
</ol>
<pre><code class="language-bash">helm install stable/cert-manager \
--name cert-manager \
--namespace kube-system \
--version v0.5.2
</code></pre>
<ol start="16">
<li>å®‰è£…rancher</li>
</ol>
<pre><code class="language-bash">helm install rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=rancher.xxx.top
</code></pre>
<pre><code class="language-bash">kubectl -n cattle-system get pod
</code></pre>
<ol start="17">
<li>å®‰è£…å®Œæˆå•¦ï¼Œè®¿é—® Rancher-UI</li>
</ol>
<pre><code class="language-bash">(èµ„æ–™æ‰¾åˆ°ï¼Œä½†æ­¤é—®é¢˜ä¼šè‡ªè¡Œæ¶ˆå¤±ï¼Œadminç”¨æˆ·å·²ç»è¢«æ·»åŠ )
è®¿é—®ï¼š https://rancher.xxx.top/ ä¸º adminè´¦æˆ·è®¾ç½®åˆå§‹å¯†ç ï¼Œå¹¶ç™»å…¥ç³»ç»Ÿã€‚æç¤ºè®¾ç½®server-urlï¼Œç¡®ä¿ä½ çš„åœ°å€æ— è¯¯ï¼Œç¡®è®¤å³å¯ã€‚éšåç¨ç­‰ä¸€ä¼šå„¿ï¼Œå¾…ç³»ç»Ÿå®Œæˆåˆå§‹åŒ–ã€‚
å¦‚æœå‡ºç°localé›†ç¾¤ä¸€ç›´åœç•™åœ¨ç­‰å¾…çŠ¶æ€ï¼Œå¹¶æç¤º Waiting for server-url setting to be setï¼Œå¯ä»¥å°è¯•ç‚¹å‡» å…¨å±€-&gt;local-&gt;å‡çº§-&gt;æ·»åŠ ä¸€ä¸ªæˆå‘˜è§’è‰²ï¼ˆadmin/ClusterOwnerï¼‰-&gt;ä¿å­˜å³å¯ã€‚
</code></pre>
<ol start="18">
<li>é—®é¢˜æ’æŸ¥</li>
</ol>
<ul>
<li>
<p>å¯åŠ¨ålocalé›†ç¾¤å­˜åœ¨(v2.2.8)</p>
<pre><code class="language-bash">
Exit status 1, Error from server (Forbidden)... balabala æƒé™é—®é¢˜

#æ£€æŸ¥å‘ç° cattle-system ä¸‹é¢å°±åªæœ‰ rancher
[rke@rancher-1 ~]$ kubectl get pod -n cattle-system
NAME                       READY   STATUS    RESTARTS   AGE
rancher-797f8646f6-4bcr9   1/1     Running   1          19m
rancher-797f8646f6-fph2c   1/1     Running   2          19m
rancher-797f8646f6-fvtnf   1/1     Running   2          19m

#å°è¯•é‡æ–°å¯åŠ¨rancherå®¹å™¨...(æ­¤æœŸé—´ï¼ŒUIæ— æ³•è®¿é—®)
kubectl -n cattle-system scale --replicas=0 deploy rancher
kubectl -n cattle-system scale --replicas=3 deploy rancher
</code></pre>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Postgreså¸¸ç”¨æ“ä½œ]]></title>
        <id>https://gois.top/post/postgres-chang-yong-cao-zuo</id>
        <link href="https://gois.top/post/postgres-chang-yong-cao-zuo">
        </link>
        <updated>2019-07-30T09:24:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="æŸ¥çœ‹åº“æ‰€æœ‰è¡¨å¤§å°">æŸ¥çœ‹åº“æ‰€æœ‰è¡¨å¤§å°</h2>
<pre><code class="language-sql">--æ•°æ®åº“ä¸­å•ä¸ªè¡¨çš„å¤§å°ï¼ˆä¸åŒ…å«ç´¢å¼•ï¼‰
select pg_size_pretty(pg_relation_size('è¡¨å'));

--æŸ¥å‡ºæ‰€æœ‰è¡¨ï¼ˆåŒ…å«ç´¢å¼•ï¼‰å¹¶æ’åº
SELECT table_schema || '.' || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size('&quot;' || table_schema || '&quot;.&quot;' || table_name || '&quot;')) AS size
FROM information_schema.tables
ORDER BY
pg_total_relation_size('&quot;' || table_schema || '&quot;.&quot;' || table_name || '&quot;') DESC limit 20

--æŸ¥å‡ºè¡¨å¤§å°æŒ‰å¤§å°æ’åºå¹¶åˆ†ç¦»dataä¸index
SELECT
table_name,
pg_size_pretty(table_size) AS table_size,
pg_size_pretty(indexes_size) AS indexes_size,
pg_size_pretty(total_size) AS total_size
FROM (
SELECT
table_name,
pg_table_size(table_name) AS table_size,
pg_indexes_size(table_name) AS indexes_size,
pg_total_relation_size(table_name) AS total_size
FROM (
SELECT ('&quot;' || table_schema || '&quot;.&quot;' || table_name || '&quot;') AS table_name
FROM information_schema.tables
) AS all_tables
ORDER BY total_size DESC
) AS pretty_sizes
</code></pre>
<h2 id="è®¾ç½®å…³é—­postgresé»˜è®¤çš„åˆ†é¡µæŸ¥è¯¢">è®¾ç½®å…³é—­postgresé»˜è®¤çš„åˆ†é¡µæŸ¥è¯¢</h2>
<pre><code>\set pager off
</code></pre>
<h2 id="å”¯ä¸€keyé‡å¤æ•°æ®æŸ¥è¯¢">å”¯ä¸€keyé‡å¤æ•°æ®æŸ¥è¯¢</h2>
<pre><code>select * from health_report where exists(select null from (select client_id,ukey from health_report group by client_id,ukey having count(*) &gt; 1)tbl_temp where (tbl_temp.client_id=health_report.client_id and tbl_temp.ukey=health_report.ukey) or (tbl_temp.client_id is null and health_report.client_id is null) or (tbl_temp.ukey is null and health_report.ukey is null)) ;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://gois.top/post/hello-gridea</id>
        <link href="https://gois.top/post/hello-gridea">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></content>
    </entry>
</feed>